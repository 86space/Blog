<!doctype html><html lang=en-ZH dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="自然语言处理（NLP）是一门融合了计算机科学、人工智能和语言学的交叉学科，旨在让计算机能够理解、解释和生成人类的自然语言。自然语言是指日常使用的语言，如汉语和英语，它比编程语言更复杂多样。 NLP的目标是使机器能够像人一样理解和使用语言，以实现更自然高效的人机交互。这包括文本信息提取、自动翻译、情感分析、语音识别和问答系统等应用。例如，NLP可以用来开发聊天机器人和翻译软件。 为了实现这些目标，研究人员使用大量数据集和先进算法，如循环神经网络（RNN）、长短时记忆网络（LSTM）和变换器（Transformer）。这些技术的进步显著提升了NLP系统的性能，在许多任务上达到了接近甚至超过人类的水平。\n"><title>人工智能：什么是自然语言处理</title>
<link rel=canonical href=https://blog.8688886.xyz/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="人工智能：什么是自然语言处理"><meta property='og:description' content="自然语言处理（NLP）是一门融合了计算机科学、人工智能和语言学的交叉学科，旨在让计算机能够理解、解释和生成人类的自然语言。自然语言是指日常使用的语言，如汉语和英语，它比编程语言更复杂多样。 NLP的目标是使机器能够像人一样理解和使用语言，以实现更自然高效的人机交互。这包括文本信息提取、自动翻译、情感分析、语音识别和问答系统等应用。例如，NLP可以用来开发聊天机器人和翻译软件。 为了实现这些目标，研究人员使用大量数据集和先进算法，如循环神经网络（RNN）、长短时记忆网络（LSTM）和变换器（Transformer）。这些技术的进步显著提升了NLP系统的性能，在许多任务上达到了接近甚至超过人类的水平。\n"><meta property='og:url' content='https://blog.8688886.xyz/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/'><meta property='og:site_name' content='八  六'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='深度学习'><meta property='article:tag' content='人工智能'><meta property='article:tag' content='机器学习'><meta property='article:tag' content='大模型'><meta property='article:tag' content='AI'><meta property='article:tag' content='自然语言处理'><meta property='article:published_time' content='2025-02-07T11:43:04+00:00'><meta property='article:modified_time' content='2025-02-07T11:43:04+00:00'><meta property='og:image' content='https://blog.8688886.xyz/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-3.png'><meta name=twitter:title content="人工智能：什么是自然语言处理"><meta name=twitter:description content="自然语言处理（NLP）是一门融合了计算机科学、人工智能和语言学的交叉学科，旨在让计算机能够理解、解释和生成人类的自然语言。自然语言是指日常使用的语言，如汉语和英语，它比编程语言更复杂多样。 NLP的目标是使机器能够像人一样理解和使用语言，以实现更自然高效的人机交互。这包括文本信息提取、自动翻译、情感分析、语音识别和问答系统等应用。例如，NLP可以用来开发聊天机器人和翻译软件。 为了实现这些目标，研究人员使用大量数据集和先进算法，如循环神经网络（RNN）、长短时记忆网络（LSTM）和变换器（Transformer）。这些技术的进步显著提升了NLP系统的性能，在许多任务上达到了接近甚至超过人类的水平。\n"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://blog.8688886.xyz/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-3.png'><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_5acc8774b0588ff.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🍥</span></figure><div class=site-meta><h1 class=site-name><a href=/>八 六</a></h1><h2 class=site-description>👩‍🦽🧑‍🦽🧑‍🦽‍➡️👩‍🦽‍➡️</h2></div></header><ol class=menu-social><li><a href=https://github.com/86space target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://t.me/blog86_bot target=_blank title=Telegram rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><ol><li><a href=#transformer模型>Transformer模型</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-3_hu_b336fd307907a5a4.png srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-3_hu_b336fd307907a5a4.png 800w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-3_hu_9da3e16f32abe71f.png 1600w" width=800 height=535 loading=lazy alt="Featured image of post 人工智能：什么是自然语言处理"></a></div><div class=article-details><header class=article-category><a href=/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/>人工智能</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/>人工智能：什么是自然语言处理</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Feb 07, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 8 分钟</time></div></footer></div></header><section class=article-content><p>自然语言处理（NLP）是一门融合了计算机科学、人工智能和语言学的交叉学科，旨在让计算机能够理解、解释和生成人类的自然语言。自然语言是指日常使用的语言，如汉语和英语，它比编程语言更复杂多样。
NLP的<strong>目标是使机器能够像人一样理解和使用语言，以实现更自然高效的人机交互。这包括文本信息提取、自动翻译、情感分析、语音识别和问答系统等应用</strong>。例如，NLP可以用来开发聊天机器人和翻译软件。
为了实现这些目标，研究人员使用大量数据集和先进算法，如循环神经网络（RNN）、长短时记忆网络（LSTM）和变换器（Transformer）。这些技术的进步显著提升了NLP系统的性能，在许多任务上达到了接近甚至超过人类的水平。</p><p><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image.png width=2418 height=794 srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image_hu_de7b7548d53384b9.png 480w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image_hu_f8bf14b38f556e24.png 1024w" loading=lazy alt=自然语言处理 class=gallery-image data-flex-grow=304 data-flex-basis=730px></p><blockquote><p>一个典型的 NLP 流程大致分为以下几个步骤：</p><ol><li><strong>输入 (Text File)：</strong> NLP 过程的起始点，通常是文本文件，例如文章、对话、网页等。</li></ol><ul><li>这是 NLP 系统的原始数据，例如一段文字：“今天天气真好，适合出去玩。”</li></ul><ol><li><strong>预处理 (Preprocessing)：</strong> 对原始文本进行清洗和规范化，为后续处理做准备。</li></ol><ul><li><p><strong>目的</strong>： 清理和规范化文本，去除噪声，使模型更容易学习。</p></li><li><p><strong>常用方法：</strong></p></li><li><ul><li><strong>分词 (Tokenization)：</strong> 将文本分解成独立的词语或子词。例如，将“今天天气真好” 分成 “今天”，“天气”，“真”，“好”。</li><li><strong>去除停用词 (Stop Word Removal)：</strong> 去除常用的、没有实际意义的词语，例如“的”，“是”，“了”等。</li><li><strong>词干提取 (Stemming) / 词形还原 (Lemmatization)：</strong> 将词语还原为原型，例如将“running” 还原为 “run”。</li><li><strong>大小写转换 (Lowercasing)：</strong> 将所有字母转换为小写，避免大小写带来的差异。</li><li><strong>标点符号去除 (Punctuation Removal)：</strong> 去除文本中的标点符号。</li></ul></li><li><p><strong>例子：</strong> 经过预处理后，“今天天气真好，适合出去玩。” 可能变为 [&ldquo;今天&rdquo;, &ldquo;天气&rdquo;, &ldquo;真&rdquo;, &ldquo;好&rdquo;, &ldquo;适合&rdquo;, &ldquo;出去玩&rdquo;]。</p></li></ul><ol><li><strong>词嵌入 (Dense Embedding)：</strong> 将文本转换为计算机可以理解的数值表示形式。<strong>一个分词的词嵌入里有好几百个值</strong></li></ol><ul><li><p><strong>目的：</strong> 将词语转换为计算机可以理解的数值向量。</p></li><li><p><strong>原理：</strong> 每个词语都被映射到一个高维空间中的向量，向量之间的距离反映了词语之间的语义相似度。</p></li><li><p><strong>常用方法：</strong></p></li><li><ul><li><strong>Word2Vec：</strong> 一种经典的词嵌入方法，通过训练神经网络来学习词向量。</li><li><strong>GloVe：</strong> 一种基于词语共现统计的词嵌入方法。</li><li><strong>FastText：</strong> 一种改进的词嵌入方法，可以处理未登录词。</li></ul></li><li><p><strong>例子：</strong> “今天” 可能被表示为 [0.1, 0.2, -0.3, 0.5 &mldr;], “天气” 可能被表示为 [0.2, 0.3, -0.1, 0.4 &mldr;]。</p></li><li><p><strong>图中 “Dense Embedding” 下面的蓝色圆点就代表了词向量。</strong></p></li><li><p><strong>词嵌入层的作用类似于查字典，将文字转换为计算机可以处理的数字，同时保留了词语的语义信息。</strong></p></li></ul><ol><li><strong>深度学习 (Deep Learning)：</strong> 使用深度神经网络来学习文本的特征和模式。</li></ol><ul><li><p><strong>目的：</strong> 利用神经网络来学习文本的深层特征和模式。</p></li><li><p><strong>常用模型：</strong></p></li><li><ul><li><strong>循环神经网络 (RNN)：</strong> 适用于处理序列数据，如文本。</li><li><strong>长短期记忆网络 (LSTM)：</strong> 改进的 RNN，可以更好地处理长距离依赖关系。</li><li><strong>Transformer：</strong> 基于自注意力机制的模型，在很多 NLP 任务上都取得了SOTA效果。</li></ul></li><li><p><strong>图中 “Hidden Layer” 代表了神经网络的隐藏层，它负责学习输入数据的复杂特征。</strong></p></li><li><p><strong>“Output Units” 代表了神经网络的输出层，它根据任务的需求输出不同的结果。</strong></p></li></ul><ol><li><strong>输出 (Output)：</strong> 根据不同的任务，输出相应的结果，例如情感分析、文本分类、实体识别、机器翻译等。</li></ol><ul><li><p><strong>目的：</strong> 根据不同的 NLP 任务，输出相应的结果。</p></li><li><p><strong>常见任务：</strong></p></li><li><ul><li><strong>情感分析 (Sentiment)：</strong> 判断文本的情感倾向，例如“积极”、“消极”、“中性”。</li><li><strong>文本分类 (Classification)：</strong> 将文本归类到预定义的类别中。</li><li><strong>实体识别 (Entity Extraction)：</strong> 识别文本中的命名实体，例如人名、地名、组织机构名。</li><li><strong>机器翻译 (Translation)：</strong> 将一种语言的文本翻译成另一种语言。</li><li><strong>主题建模 (Topic Modeling)：</strong> 从文本中提取主题。</li></ul></li></ul></blockquote><p><strong>词嵌入扩展</strong></p><p>处理分词，模型会生成一个包含多个值的向量（或者说一列数值），并根据每个单词与 work 在训练数据中的距离调整这些值。这个向量就是所谓的词嵌入（Word Embedding）。</p><p><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-1.png width=623 height=561 srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-1_hu_bda9a927600d8d81.png 480w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-1_hu_3a7bdb6ec5455a9b.png 1024w" loading=lazy alt=词嵌入-1 class=gallery-image data-flex-grow=111 data-flex-basis=266px></p><p>词嵌入里有好几百个值，每个值都代表了单词意义的不同方面。就好比我们用类型、位置、卧室数、浴室数和楼层数来描述房子一样，词嵌入里的值描述了单词的语言特性。</p><p><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-2.png width=1362 height=195 srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-2_hu_8b2e48f1258f5111.png 480w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-2_hu_7111fa664ebf6980.png 1024w" loading=lazy alt=词嵌入-2 class=gallery-image data-flex-grow=698 data-flex-basis=1676px></p><p>由于我们无法准确知道每个值代表什么，有意思的是，我们发现那些意思相近的单词，它们的词嵌入往往很像。比如 <strong>sea</strong> 和 <strong>ocean</strong>，虽然它们不能在所有情境下互换，但它们的意思很接近，通过词嵌入，我们能量化这种相似度。</p><p><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-3.png width=1073 height=718 srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-3_hu_55ba1f912ff1770f.png 480w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-3_hu_9b6883ea8beb992a.png 1024w" loading=lazy alt=词嵌入-3 class=gallery-image data-flex-grow=149 data-flex-basis=358px></p><p>如果我们简化词嵌入，只用两个值来表示，就能更直观地看到单词之间的“距离”或者说相似度了。这样，我们就能发现一些词群，比如 <strong>代词</strong>群， <strong>交通工具</strong>群。而能量化单词，正是让模型成功生成文本的第一步。</p><p><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-4.png width=1280 height=1672 srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-4_hu_e73b082821830dd2.png 480w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-4_hu_541b133b180911.png 1024w" loading=lazy alt=词嵌入-4 class=gallery-image data-flex-grow=76 data-flex-basis=183px></p><p>接下来学习最富盛名的自然语言框架：Transformer模型，也就是隐藏层和输出层的内容。</p><h3 id=transformer模型>Transformer模型</h3><p>Transformer 模型的核心思想是使用**自注意力机制（Self-Attention Mechanism）**来捕捉输入序列中不同位置之间的关系，而不是像 RNN 那样按顺序处理。自注意力机制允许模型同时关注输入序列中的所有位置，从而更好地捕捉长距离依赖关系。</p><p>Transformer的内部，在本质上是一个 encoder-decoder的结构，即编码器-解码器</p><p><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-5.png width=1518 height=1044 srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-5_hu_91ab61f5aa52de63.png 480w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-5_hu_fbc1d5f56fb11ec3.png 1024w" loading=lazy alt=Transformer模型 class=gallery-image data-flex-grow=145 data-flex-basis=348px></p><p>Transformer工作流程</p><p><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-6.png width=1156 height=1706 srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-6_hu_7754c7214245fca0.png 480w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-6_hu_b574cb11e76db23b.png 1024w" loading=lazy alt=Transformer工作流程 class=gallery-image data-flex-grow=67 data-flex-basis=162px></p><blockquote><p><strong>1. 整体结构：</strong></p><ol><li><strong>输入 (Inputs)：</strong> 模型的原始输入，例如文本序列。</li><li><strong>输入嵌入 (Input Embedding)：</strong> 将输入的词语转换为词向量表示。</li><li><strong>位置编码 (Positional Encoding)：</strong> 为词向量添加位置信息，以表示词语在序列中的顺序。</li><li><strong>编码器 (Encoder)：</strong> 负责处理输入序列，提取文本的语义信息。</li><li><strong>解码器 (Decoder)：</strong> 负责根据编码器的输出，生成目标序列。</li><li><strong>输出 (Outputs)：</strong> 模型的最终输出，例如翻译后的文本，或者分类结果。</li><li><strong>输出概率 (Output Probabilities)：</strong> 对输出进行概率化处理，以便进行概率预测。</li></ol><p><strong>2. 具体步骤详解：</strong></p><ul><li><p><strong>输入 (Inputs)：</strong></p></li><li><ul><li>图中显示输入为 “我 是 一个 学生”，这可以是任何文本序列，例如一句话、一篇文章等。</li></ul></li><li><p><strong>输入嵌入 (Input Embedding)：</strong></p></li><li><ul><li><strong>目的：</strong> 将输入的词语转换为计算机可以处理的数值向量，即词向量。</li><li><strong>过程：</strong> 每个词语通过一个嵌入层（Embedding Layer）映射到一个高维向量空间，例如，“我” 可能被表示为 [0.1, 0.2, -0.3, 0.5 &mldr;]。</li><li><strong>图中， “Input Embedding” 模块负责完成这个转换。</strong></li></ul></li><li><p><strong>位置编码 (Positional Encoding)：</strong></p></li><li><ul><li><strong>目的：</strong> 由于 Transformer 模型没有 RNN 的顺序处理机制，需要额外的位置编码来表示词语在句子中的位置信息。</li><li><strong>过程：</strong> 位置编码通过一个特定的函数生成，并与词向量相加，从而将位置信息融入到词向量中。</li><li><strong>图中， “Positional Encoding” 模块负责生成位置编码，并与词向量相加。</strong></li></ul></li><li><p><strong>编码器 (Encoder)：</strong></p></li><li><ul><li><strong>目的：</strong> 提取输入序列的语义信息。</li><li><strong>结构：</strong> 由 Nx 个相同的编码器层堆叠而成（图中显示 Nx）。每一层编码器包含：</li></ul></li><li><ul><li><ul><li><strong>多头注意力层 (Multi-Head Attention)：</strong> 这是 Transformer 模型的核心，它使用自注意力机制来计算输入序列中不同位置之间的关系。</li><li><strong>前馈神经网络 (Feed Forward)：</strong> 对每个位置的输出进行非线性变换。</li><li><strong>加和 & 归一化 (Add & Norm)：</strong> 使用残差连接和层归一化来稳定训练过程，加速收敛。</li></ul></li></ul></li><li><ul><li><strong>图中，橙色框代表编码器，它接收经过位置编码的输入，并通过多层注意力机制和前馈网络提取特征。</strong></li><li><strong>编码器的输出是包含了输入序列语义信息的向量表示。</strong></li></ul></li><li><p><strong>解码器 (Decoder)：</strong></p></li><li><ul><li><strong>目的：</strong> 根据编码器的输出，生成目标序列。</li><li><strong>结构：</strong> 由 Nx 个相同的解码器层堆叠而成（图中显示 Nx）。每一层解码器包含：</li></ul></li><li><ul><li><ul><li><strong>掩码多头注意力层 (Masked Multi-Head Attention)：</strong> 与编码器的多头注意力层类似，但只关注解码器当前位置之前的信息，避免信息泄露。</li><li><strong>多头注意力层 (Multi-Head Attention)：</strong> 计算解码器当前位置与编码器输出之间的注意力权重，用于将编码器的信息融入到解码器中。</li><li><strong>前馈神经网络 (Feed Forward)：</strong> 与编码器中的前馈网络类似。</li><li><strong>加和 & 归一化 (Add & Norm)：</strong> 与编码器中的残差连接和层归一化类似。</li></ul></li></ul></li><li><ul><li><strong>图中，粉色框代表解码器，它接收编码器的输出，并通过多层注意力机制和前馈网络生成目标序列。</strong></li><li><strong>解码器是自回归的，它逐词生成目标序列。</strong></li></ul></li><li><p><strong>输出嵌入 (Output Embedding)：</strong></p></li><li><ul><li><strong>目的：</strong> 将目标序列的词语转换为词向量。</li><li><strong>过程：</strong> 与输入嵌入类似，每个目标词语通过一个嵌入层映射到一个高维向量空间。</li><li><strong>图中，“Output Embedding” 模块负责目标词语的向量化。</strong></li><li><strong>在训练过程中，解码器的输入是目标序列，但向右移动一位，第一个输入是起始符</strong> <code>**&lt;start>**</code><strong>，最后一个输入是结束符</strong> <code>**&lt;end>**</code><strong>。</strong></li></ul></li><li><p><strong>线性层 (Linear)：</strong></p></li><li><ul><li><strong>目的：</strong> 将解码器的输出转换为与词汇表大小相同的向量，用于计算每个词语的概率。</li><li><strong>过程：</strong> 通过一个线性变换，将解码器的输出映射到一个词汇表大小的空间。</li></ul></li><li><p><strong>Softmax：</strong></p></li><li><ul><li><strong>目的：</strong> 将线性层的输出转换为概率分布，用于预测下一个词语。</li><li><strong>过程：</strong> Softmax 函数将线性层的输出转换为一个概率分布，每个位置的值表示该位置是词汇表中某个词语的概率。</li></ul></li><li><p><strong>输出概率 (Output Probabilities)：</strong></p></li><li><ul><li><strong>目的：</strong> 表示模型预测的下一个词语的概率分布。</li><li><strong>图中，输出显示了生成目标序列 “i am a student” 的过程，每个词语都对应一个概率分布。</strong></li><li><strong>例如，第一个输出 “i” 的概率最高，第二个输出 “am” 的概率最高，以此类推。</strong></li></ul></li></ul></blockquote><p><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-7.png width=940 height=207 srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-7_hu_552aaed610e647f0.png 480w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-7_hu_6df0452228c2173a.png 1024w" loading=lazy alt=HF class=gallery-image data-flex-grow=454 data-flex-basis=1089px></p></section><footer class=article-footer><section class=article-tags><a href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a>
<a href=/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/>人工智能</a>
<a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a>
<a href=/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/>大模型</a>
<a href=/tags/ai/>AI</a>
<a href=/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/>自然语言处理</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>自由、和谐、民主、富强 🤯 2025</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/><div class=article-image><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-1.8fc57a9501522283bc62ca0676bf9108_hu_33876816c1fc9349.png width=250 height=150 loading=lazy alt="Featured image of post 人工智能：什么是神经网络" data-hash="md5-j8V6lQFSIoO8YsoGdr+RCA=="></div><div class=article-details><h2 class=article-title>人工智能：什么是神经网络</h2></div></a></article><article class=has-image><a href=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/><div class=article-image><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-3.f38c2eb2fa13428ff5152a29f7b79025_hu_c0cc08818ce2ab8a.png width=250 height=150 loading=lazy alt="Featured image of post 人工智能：什么是深度学习" data-hash="md5-84wusvoTQo/1FSop97eQJQ=="></div><div class=article-details><h2 class=article-title>人工智能：什么是深度学习</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 八 六</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>