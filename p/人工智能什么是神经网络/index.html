<!doctype html><html lang=en-ZH dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="人工神经网络（Artificial Neural Network ，简写：ANN）是模仿生物神经系统（特别是人脑）结构和功能的一种计算模型。它们是深度学习的核心组成部分，赋予了深度学习强大的模式识别和学习能力。\n"><title>人工智能：什么是神经网络</title>
<link rel=canonical href=https://blog.8688886.xyz/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="人工智能：什么是神经网络"><meta property='og:description' content="人工神经网络（Artificial Neural Network ，简写：ANN）是模仿生物神经系统（特别是人脑）结构和功能的一种计算模型。它们是深度学习的核心组成部分，赋予了深度学习强大的模式识别和学习能力。\n"><meta property='og:url' content='https://blog.8688886.xyz/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/'><meta property='og:site_name' content='八  六'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='深度学习'><meta property='article:tag' content='人工智能'><meta property='article:tag' content='机器学习'><meta property='article:tag' content='大模型'><meta property='article:tag' content='AI'><meta property='article:published_time' content='2025-02-07T11:30:28+00:00'><meta property='article:modified_time' content='2025-02-07T11:30:28+00:00'><meta property='og:image' content='https://blog.8688886.xyz/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-1.png'><meta name=twitter:title content="人工智能：什么是神经网络"><meta name=twitter:description content="人工神经网络（Artificial Neural Network ，简写：ANN）是模仿生物神经系统（特别是人脑）结构和功能的一种计算模型。它们是深度学习的核心组成部分，赋予了深度学习强大的模式识别和学习能力。\n"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://blog.8688886.xyz/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-1.png'><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_5acc8774b0588ff.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🍥</span></figure><div class=site-meta><h1 class=site-name><a href=/>八 六</a></h1><h2 class=site-description>👩‍🦽🧑‍🦽🧑‍🦽‍➡️👩‍🦽‍➡️</h2></div></header><ol class=menu-social><li><a href=https://github.com/86space target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://t.me/blog86_bot target=_blank title=Telegram rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><ol><li><a href=#激活函数>激活函数</a></li><li><a href=#损失函数>损失函数</a></li><li><a href=#网络优化方法>网络优化方法</a></li><li><a href=#前向传播和反向传播>前向传播和反向传播</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-1_hu_b60f6ba4f2b01cd7.png srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-1_hu_b60f6ba4f2b01cd7.png 800w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-1_hu_76a55c4145270781.png 1600w" width=800 height=417 loading=lazy alt="Featured image of post 人工智能：什么是神经网络"></a></div><div class=article-details><header class=article-category><a href=/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/>人工智能</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>人工智能：什么是神经网络</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Feb 07, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 12 分钟</time></div></footer></div></header><section class=article-content><p>人工神经网络（Artificial Neural Network ，简写：ANN）是模仿生物神经系统（特别是人脑）结构和功能的一种计算模型。它们是深度学习的核心组成部分，赋予了深度学习强大的模式识别和学习能力。</p><p><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image.png width=1125 height=1277 srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image_hu_645a7a95ef806ba5.png 480w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image_hu_dc084fea3dc550ee.png 1024w" loading=lazy alt=神经末梢 class=gallery-image data-flex-grow=88 data-flex-basis=211px></p><p><strong>神经网络的基本结构：</strong></p><p>一个基本的神经网络由以下几个部分组成：</p><ul><li><p><strong>神经元（Neuron）：</strong> 也称为节点，是神经网络的基本单元。每个神经元接收输入，进行某种处理，然后产生输出。</p></li><li><p><strong>连接（Connection）：</strong> 神经元之间通过连接相互传递信息。每个连接都有一个权重（Weight），表示连接的强度。权重决定了输入信号对输出的影响程度。</p></li><li><p><strong>层（Layer）：</strong> 神经元按层组织。一个典型的神经网络包括：</p></li><li><ul><li><strong>输入层（Input Layer）：</strong> 接收外部输入数据。</li><li><strong>隐藏层（Hidden Layer）：</strong> 位于输入层和输出层之间，负责对输入数据进行处理和转换。深度学习中的“深度”就体现在这里，拥有多个隐藏层。</li><li><strong>输出层（Output Layer）：</strong> 产生最终的输出结果。</li></ul></li><li><p><strong>特点</strong>：</p></li><li><ul><li>同一层的神经元之间没有连接。</li><li>第N层的每个神经元和第N-1层的所有神经元相连（这就是full connection的含义），也称为全连接神经网络。</li><li>第N-1层神经元的输出就是第N层神经元的输入。</li><li>每个连接都有一个权重值（w系数和b系数）。</li></ul></li></ul><p><strong>神经网络的工作原理：</strong></p><ol><li><strong>输入：</strong> 输入数据被传递到输入层的神经元。</li><li><strong>加权和：</strong> 每个神经元接收来自上一层神经元的输入，并将这些输入乘以相应的权重进行加权求和。</li><li><strong>激活函数：</strong> 将加权和传递给一个激活函数。激活函数引入了非线性，使得神经网络能够学习复杂的模式。常见的激活函数包括Sigmoid、ReLU、Tanh等。</li><li><strong>输出：</strong> 激活函数的输出成为该神经元的输出，并传递到下一层神经元。</li><li><strong>重复：</strong> 这个过程在网络中逐层进行，直到到达输出层。</li></ol><p><strong>训练神经网络：</strong></p><p>训练神经网络的过程包括以下几个步骤：</p><ol><li><strong>前向传播（Forward Propagation）</strong>：将输入数据通过网络各层传递，计算每个神经元的输出，直到产生最终输出。</li><li><strong>损失函数（Loss Function）</strong>：计算预测输出和实际标签之间的误差。这种误差称为损失，常见的损失函数有均方误差（MSE）和交叉熵损失（Cross-Entropy Loss）。</li><li><strong>反向传播（Backpropagation）</strong>：通过损失函数的梯度计算，反向调整每个神经元的权重，以最小化损失。反向传播算法使用链式法则来高效计算梯度。</li><li><strong>优化器（Optimizer）</strong>：使用优化算法（如梯度下降、Adam 等）根据计算出的梯度更新网络权重，从而减小损失。</li></ol><p><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-1.png width=1125 height=587 srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-1_hu_599f0df71dabbc9a.png 480w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-1_hu_2d163e04ebcffbcc.png 1024w" loading=lazy alt=神经网络样例 class=gallery-image data-flex-grow=191 data-flex-basis=459px></p><h3 id=激活函数>激活函数</h3><p>加权和在进行输入计算后始终是<strong>线性</strong>的，需要引入激活函数用于对每层的输出数据进行变换，进而为整个网络注入非线性因素，此时神经网络可以拟合出各种曲线，以此来表达复杂问题的解答。</p><p><strong>Pytorch中封装了常见的激活函数</strong></p><p><strong>sigmoid 激活函数&ndash; 指数型</strong></p><p><code>sigmoid</code>函数一般只用于<strong>二分类</strong>的<strong>输出层</strong></p><p><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-2.png width=1125 height=642 srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-2_hu_109efa805cc9bc9f.png 480w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-2_hu_3fe41a6e746abaaa.png 1024w" loading=lazy alt="sigmoid 激活函数" class=gallery-image data-flex-grow=175 data-flex-basis=420px></p><p>Sigmoid函数，也称为S型函数或乙状函数，是一种形状像字母“S”的数学函数。它在机器学习，特别是深度学习和逻辑回归中，作为激活函数被广泛使用。</p><p><strong>定义和公式：</strong></p><p>Sigmoid函数最常见的形式是<strong>逻辑斯谛函数</strong>，其公式如下：</p><p>σ(x) = 1 / (1 + e<sup>-x</sup>)</p><p>其中：</p><ul><li>σ(x) 是函数的输出值。</li><li>x 是函数的输入值（可以是任意实数）。</li><li>e 是自然常数（约等于2.71828）。</li></ul><p><strong>图像和特性：</strong></p><p>Sigmoid函数的图像呈S形，具有以下特性：</p><ul><li><strong>定义域：</strong> (−∞, +∞) （输入可以是任意实数）</li><li><strong>值域：</strong> (0, 1) （输出值始终在0和1之间，但不包括0和1）</li><li><strong>单调递增：</strong> 输入值越大，输出值也越大。</li><li><strong>连续光滑：</strong> 函数曲线平滑连续，处处可导。</li><li><strong>在x=0处中心对称：</strong> σ(0) = 0.5。</li><li><strong>导数：</strong> σ&rsquo;(x) = σ(x)(1 - σ(x))，导数可以用函数自身来表示，这在反向传播算法中非常有用。</li></ul><p><strong>Sigmoid函数在神经网络中的应用：</strong></p><p>在神经网络中，Sigmoid函数通常作为激活函数使用，其主要作用是引入非线性。如果没有激活函数，神经网络就只能进行线性运算，表达能力非常有限。Sigmoid函数将神经元的加权输入转换为0到1之间的输出，可以看作是神经元“激活”的概率。</p><p><strong>Sigmoid函数的优缺点：</strong></p><p><strong>优点：</strong></p><ul><li><strong>输出范围有限：</strong> 将输出限制在0到1之间，可以方便地表示概率或进行归一化。</li><li><strong>光滑可导：</strong> 方便进行梯度计算，用于反向传播算法。</li><li><strong>易于理解和使用：</strong> 函数形式简单，易于实现。</li></ul><p><strong>缺点：</strong></p><ul><li><strong>梯度消失：</strong> 当输入值非常大或非常小时，函数的梯度接近于0。这会导致在反向传播过程中，梯度难以传递到前面的层，从而导致网络训练缓慢甚至停滞。这是Sigmoid函数最大的缺点，也是它在深度神经网络中逐渐被其他激活函数（如ReLU）取代的主要原因。</li><li><strong>输出不是以0为中心：</strong> 输出值始终为正，这可能会导致一些优化问题。</li><li><strong>计算量较大：</strong> 计算指数运算相对耗时。</li></ul><p><strong>ReLU激活函数-分段型</strong></p><p><code>ReLu</code>函数常用于<strong>隐藏层</strong></p><p><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-3.png width=1125 height=568 srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-3_hu_c607993782f946d0.png 480w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-3_hu_b97926b737639c65.png 1024w" loading=lazy alt=ReLU激活函数 class=gallery-image data-flex-grow=198 data-flex-basis=475px></p><p><strong>定义和公式：</strong></p><p>ReLU函数的数学表达式非常简单：</p><p>f(x) = max(0, x)</p><p>这意味着：</p><ul><li>当输入x大于0时，输出等于输入本身（f(x) = x）。</li><li>当输入x小于等于0时，输出为0。</li></ul><p><strong>图像和特性：</strong></p><p>ReLU函数的图像由两条直线组成，在x=0处有一个拐点。其主要特性包括：</p><ul><li><strong>非线性：</strong> ReLU函数虽然形式简单，但它是一个非线性函数，这使得神经网络能够学习复杂的模式。</li><li><strong>计算简单：</strong> ReLU函数只需要进行简单的比较和赋值操作，计算速度非常快。</li><li><strong>单侧抑制：</strong> 当输入小于0时，输出恒为0，这导致一部分神经元处于“非激活”状态，有助于网络的稀疏性表达，减少参数之间的相互依赖，缓解过拟合。</li><li><strong>不存在梯度消失问题（在正区间）：</strong> 当输入大于0时，梯度恒为1，这使得梯度可以有效地传递到前面的层，避免了梯度消失问题，加快了网络训练速度。</li></ul><p><strong>ReLU函数的优缺点：</strong></p><p><strong>优点：</strong></p><ul><li><strong>缓解梯度消失问题：</strong> 这是ReLU最重要的优点之一，它有效地解决了Sigmoid和tanh等激活函数在输入较大或较小时容易出现的梯度消失问题，使得深度神经网络更容易训练。</li><li><strong>计算速度快：</strong> 相比于Sigmoid和tanh等需要进行指数运算的激活函数，ReLU的计算速度非常快，这大大加快了网络的训练速度。</li><li><strong>促进稀疏性：</strong> ReLU的单侧抑制特性使得一部分神经元输出为0，从而产生稀疏的网络结构，有助于减少参数之间的相互依赖，提高模型的泛化能力。</li></ul><p><strong>缺点：</strong></p><ul><li><strong>Dead ReLU Problem（神经元死亡问题）：</strong> 如果某个神经元的输入在训练过程中一直为负数，那么该神经元的输出将始终为0，导致该神经元“死亡”，不再对网络的学习起到作用。这通常是由于较大的学习率或不合适的初始化参数导致的。</li></ul><p><strong>ReLU的变体：</strong></p><p>为了解决Dead ReLU Problem，人们提出了一些ReLU的变体，例如：</p><ul><li><strong>Leaky ReLU（带泄漏的ReLU）：</strong> 将输入小于0的部分赋予一个很小的斜率，而不是直接设为0。例如，f(x) = x (x>0)；f(x) = αx (x&lt;=0)，其中α是一个很小的常数，例如0.01。这避免了神经元完全“死亡”的情况。</li><li><strong>Parametric ReLU (PReLU，参数化ReLU)：</strong> 将Leaky ReLU中的α作为一个可学习的参数，通过反向传播进行学习。</li><li><strong>Exponential Linear Unit (ELU，指数线性单元)：</strong> 在输入小于0的部分使用指数函数，而不是简单的线性函数。</li></ul><p><strong>ReLU在神经网络中的应用：</strong></p><p>ReLU及其变体是目前深度学习中最常用的激活函数之一，尤其在卷积神经网络（CNN）中应用广泛。通常来说，ReLU是隐藏层激活函数的首选。</p><p><strong>SoftMax激活函数</strong></p><p>常用于处理<strong>多分类问题</strong>的<strong>输出层</strong>，将多分类结果以<strong>概率</strong>的形式展示。</p><p><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-4.png width=1125 height=677 srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-4_hu_552f8958335d1d42.png 480w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-4_hu_285523e76caac399.png 1024w" loading=lazy alt=SoftMax激活函数 class=gallery-image data-flex-grow=166 data-flex-basis=398px></p><p><strong>定义和公式：</strong></p><p>给定一个包含 n 个实数的向量 z = (z<sub>1</sub>, z<sub>2</sub>, &mldr;, z<sub>n</sub>)，Softmax 函数的定义如下：</p><p>Softmax(z)<sub>i</sub> = e<sup>z<sub>i</sub></sup> / Σ<sub>j=1</sub><sup>n</sup> e<sup>z<sub>j</sub></sup></p><p>其中：</p><ul><li>Softmax(z)<sub>i</sub> 表示向量 z 的第 i 个元素的 Softmax 输出。</li><li>e 是自然常数（约等于 2.71828）。</li><li>Σ<sub>j=1</sub><sup>n</sup> e<sup>z<sub>j</sub></sup> 表示对向量 z 的所有元素的指数求和。</li></ul><p><strong>工作原理：</strong></p><p>Softmax 函数首先对输入向量的每个元素取指数，然后将每个元素的指数值除以所有元素指数值的总和。这样就保证了输出向量的每个元素都在 (0, 1) 之间，并且所有元素的总和为 1，从而构成了一个概率分布。</p><p><strong>Softmax 在神经网络中的应用：</strong></p><p>Softmax 函数通常用于神经网络的输出层，尤其是在多分类问题中。例如，在图像分类任务中，神经网络的输出层可以有 10 个神经元，分别对应 10 个不同的类别。经过 Softmax 函数处理后，输出层每个神经元的输出就表示该图像属于对应类别的概率。</p><p><strong>Softmax 的优点：</strong></p><ul><li><strong>输出为概率分布：</strong> Softmax 的输出可以直接解释为概率，方便进行分类决策。</li><li><strong>突出最大值：</strong> Softmax 函数能够突出输入向量中值最大的元素，使得分类结果更加明确。</li></ul><p><strong>Softmax 的缺点：</strong></p><ul><li><strong>计算开销：</strong> 需要计算指数和求和，计算开销相对较大。</li><li><strong>梯度消失（在某些情况下）：</strong> 当输入向量的某些元素值非常大时，可能会导致梯度消失问题。</li><li><strong>对输入变化敏感：</strong> 输入向量的微小变化可能会导致输出概率的较大变化。</li></ul><p><strong>Softmax 与其他激活函数的比较：</strong></p><ul><li><strong>与 Sigmoid 的比较：</strong> Sigmoid 函数通常用于二分类问题，而 Softmax 函数则用于多分类问题。当类别数为 2 时，Softmax 函数退化为 Sigmoid 函数。</li><li><strong>与 ReLU 的比较：</strong> ReLU 函数主要用于隐藏层，而 Softmax 函数主要用于输出层。ReLU 解决了 Sigmoid 的梯度消失问题，但 Softmax 仍然在多分类输出层占有重要地位。</li></ul><p><strong>总结：</strong></p><p>Softmax 激活函数是一种重要的激活函数，尤其在多分类问题中应用广泛。它将输入向量转换为概率分布，方便进行分类决策。虽然存在一些缺点，但仍然是深度学习中不可或缺的一部分。</p><p><strong>补充说明：</strong></p><ul><li>为了数值稳定性，在实际应用中，通常会对 Softmax 函数的计算进行一些改进，例如减去输入向量的最大值。</li><li>Softmax 函数通常与交叉熵损失函数一起使用，以优化神经网络的训练。</li></ul><p><strong>示例：</strong></p><p>假设有一个输入向量 z = [2, 1, 0]，则 Softmax 的计算过程如下：</p><ol><li>计算每个元素的指数：</li></ol><ul><li><ul><li>e<sup>2</sup> ≈ 7.39</li><li>e<sup>1</sup> ≈ 2.72</li><li>e<sup>0</sup> = 1</li></ul></li></ul><ol><li>计算所有元素指数值的总和：</li></ol><ul><li><ul><li>7.39 + 2.72 + 1 = 11.11</li></ul></li></ul><ol><li>计算每个元素的 Softmax 输出：</li></ol><ul><li><ul><li>Softmax(z)<sub>1</sub> = 7.39 / 11.11 ≈ 0.665</li><li>Softmax(z)<sub>2</sub> = 2.72 / 11.11 ≈ 0.245</li><li>Softmax(z)<sub>3</sub> = 1 / 11.11 ≈ 0.090</li></ul></li></ul><p>因此，Softmax(z) ≈ [0.665, 0.245, 0.090]。可以看到，这三个数的和接近于 1，并且每个数都在 0 到 1 之间。</p><p><strong>激活函数的选择方法</strong>
对于隐藏层：</p><ol><li>优先选择<code>ReLU</code>激活函数</li><li>如果<code>relu</code>效果不好，那么尝试其他激活，如<code>leaky ReLU</code>等。</li><li>如果你使用了ReLU,需要注意一下<code>dead ReLU</code>问题，避免出现大的梯度从而导致过多的神经元死亡。</li></ol><p>对于输出层：</p><ol><li>二分类问题选择<code>sigmoid</code>激活函数</li><li>多分类问题选择<code>SoftMax</code>激活函数</li></ol><p><strong>其他常见的激活函数</strong></p><p><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-5.png width=1125 height=975 srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-5_hu_cf3d7d57d21a7a9f.png 480w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-5_hu_8b2510f1e968fa40.png 1024w" loading=lazy alt=激活函数 class=gallery-image data-flex-grow=115 data-flex-basis=276px></p><h3 id=损失函数>损失函数</h3><p>什么是损失函数</p><p>用来<strong>衡量模型参数质量</strong>的<strong>函数</strong>，衡量的方式是比较神经网络输出和真实输出的差异，**损失函数告诉我们模型“犯了多大的错误”。通过最小化损失函数，我们可以优化模型的参数，使其预测结果更接近真实值。**例如：输入一张猫的照片，看输出的结果，</p><p><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-6.png width=1125 height=490 srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-6_hu_31caf0c72f811138.png 480w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-6_hu_d1cb0013b9a51a90.png 1024w" loading=lazy alt=损失函数 class=gallery-image data-flex-grow=229 data-flex-basis=551px></p><p><strong>损失函数的作用：</strong></p><ol><li><strong>指导模型训练：</strong> 在训练过程中，优化算法（如梯度下降）会根据损失函数的梯度来调整模型参数，从而使损失函数的值不断减小。</li><li><strong>评估模型性能：</strong> 在训练完成后，我们可以使用损失函数来评估模型在测试集上的表现，从而了解模型的泛化能力。</li></ol><p><strong>常见的损失函数：</strong></p><p>损失函数可以根据任务类型进行分类，主要分为以下两类：</p><ul><li><strong>回归问题损失函数：</strong> 用于预测连续值的任务。</li><li><strong>分类问题损失函数：</strong> 用于预测离散类别的任务。</li></ul><p>分类问题损失函数</p><p><strong>交叉熵损失（Cross-Entropy Loss）：</strong> 常用于多分类问题。</p><p>在多分类任务通常使用softmax将logits转换为概率的形式</p><p><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-7.png width=1125 height=605 srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-7_hu_7dd2c196c3ceb51b.png 480w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-7_hu_de14893cb9134aca.png 1024w" loading=lazy alt=交叉熵损失 class=gallery-image data-flex-grow=185 data-flex-basis=446px></p><p>H(p, q) = - Σp(x)log(q(x))</p><p>其中，p(x) 是真实概率分布，q(x) 是预测概率分布。</p><ul><li><strong>优点：</strong> 能够有效地衡量两个概率分布之间的差异，优化效果好。</li></ul><p><strong>损失函数与激活函数的关系：</strong></p><p>损失函数和激活函数是深度学习模型中两个重要的组成部分，它们之间存在一定的联系。例如，在多分类问题中，通常使用 Softmax 激活函数将输出转换为概率分布，然后使用交叉熵损失函数来衡量预测结果与真实结果之间的差异。</p><h3 id=网络优化方法>网络优化方法</h3><p>梯度下降算法是一种用于<strong>寻找损失函数最小值</strong>的优化算法</p><p><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-8.png width=1125 height=535 srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-8_hu_978617e7d22e20fd.png 480w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-8_hu_d3eed1a049b4c5e5.png 1024w" loading=lazy alt=梯度下降算法 class=gallery-image data-flex-grow=210 data-flex-basis=504px></p><p><strong>核心思想：</strong></p><p>想象你站在一座山上，想要下到山谷。梯度下降法就像你在山上寻找最陡峭的下坡路，然后沿着这个方向走一步，重复这个过程，直到到达山谷的最低点。</p><p><strong>数学原理：</strong></p><ul><li><strong>梯度：</strong> 在数学中，梯度是一个向量，它指向函数值增长最快的方向。因此，梯度的反方向就是函数值下降最快的方向。</li><li><strong>迭代：</strong> 梯度下降法通过不断迭代来逼近最小值。每次迭代，它都会沿着梯度的反方向移动一小步。</li></ul><p><strong>公式：</strong></p><p>θ<sub>new</sub> = θ<sub>old</sub> - α * ∇J(θ)</p><p>其中：</p><ul><li>θ<sub>new</sub>：更新后的参数值。</li><li>θ<sub>old</sub>：当前的参数值。</li><li>α：学习率（learning rate），控制每次迭代的步长。</li><li>∇J(θ)：损失函数 J(θ) 关于参数 θ 的梯度。</li></ul><p><strong>步骤：</strong></p><ol><li><strong>初始化参数：</strong> 随机初始化模型的参数 θ。</li><li><strong>计算梯度：</strong> 计算损失函数 J(θ) 关于参数 θ 的梯度 ∇J(θ)。</li><li><strong>更新参数：</strong> 使用上述公式更新参数 θ。</li><li><strong>重复步骤 2 和 3：</strong> 直到损失函数的值收敛到一个可接受的范围内，或者达到预定的迭代次数。</li></ol><p><strong>不同类型的梯度下降：</strong></p><p>根据每次迭代使用的样本数量，梯度下降法可以分为以下几种类型：</p><ul><li><p><strong>批量梯度下降（Batch Gradient Descent, BGD）：</strong> 每次迭代使用所有训练样本来计算梯度。</p></li><li><ul><li><strong>优点：</strong> 能够保证收敛到全局最小值（对于凸函数），训练过程相对稳定。</li><li><strong>缺点：</strong> 计算量大，训练速度慢，不适合处理大规模数据集。</li></ul></li><li><p><strong>随机梯度下降（Stochastic Gradient Descent, SGD）：</strong> 每次迭代只使用一个随机选择的训练样本来计算梯度。</p></li><li><ul><li><strong>优点：</strong> 计算速度快，适合处理大规模数据集，有可能跳出局部最小值。</li><li><strong>缺点：</strong> 训练过程波动较大，不容易收敛到全局最小值。</li></ul></li><li><p><strong>小批量梯度下降（Mini-batch Gradient Descent, MBGD）：</strong> 每次迭代使用一小部分随机选择的训练样本（称为一个 mini-batch）来计算梯度。</p></li><li><ul><li><strong>优点：</strong> 结合了 BGD 和 SGD 的优点，既能保证一定的稳定性，又能加快训练速度。</li><li><strong>缺点：</strong> 需要选择合适的 mini-batch 大小。</li></ul></li></ul><p><strong>学习率的选择：</strong></p><p>学习率 α 是梯度下降法中一个重要的超参数，它控制了每次迭代的步长。</p><ul><li><strong>学习率过大：</strong> 可能导致算法在最小值附近震荡，无法收敛。</li><li><strong>学习率过小：</strong> 可能导致收敛速度过慢。</li></ul><p>通常需要通过实验来选择合适的学习率。常用的方法包括：</p><ul><li><strong>试错法：</strong> 尝试不同的学习率，观察训练效果。</li><li><strong>学习率衰减：</strong> 随着训练的进行，逐渐减小学习率。</li></ul><p><strong>总结：</strong></p><p>梯度下降法是一种简单而有效的优化算法，在机器学习和深度学习中有着广泛的应用。理解其基本原理、不同类型以及学习率的选择对于有效地使用梯度下降法至关重要。</p><p><strong>补充说明：</strong></p><p>除了基本的梯度下降法，还有许多改进的优化算法，例如：</p><ul><li><strong>动量法（Momentum）：</strong> 引入动量来加速收敛，并减少震荡。</li><li><strong>Adam 优化器：</strong> 结合了动量法和 RMSProp 的优点，是一种常用的自适应优化算法。</li></ul><p>这些改进的优化算法通常能够更快地收敛到最小值，并且对学习率的选择不那么敏感。</p><h3 id=前向传播和反向传播>前向传播和反向传播</h3><p>前向传播 传参正向计算的过程</p><p>反向传播 参数优化的过程，调整权重参数weight，先从深层开始，逐步到浅层。</p><p><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-9.png width=1125 height=516 srcset="/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-9_hu_1a80d084c78cd41d.png 480w, /p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-9_hu_a6ff502ab77e9591.png 1024w" loading=lazy alt=前向传播和反向传播 class=gallery-image data-flex-grow=218 data-flex-basis=523px></p><p><strong>前向传播和反向传播视频教程</strong></p><div class=video-wrapper><iframe src="https://player.bilibili.com/player.html?as_wide=1&amp;high_quality=1&amp;page=1&bvid=BV1pQ4y147re" scrolling=no frameborder=no framespacing=0 allowfullscreen></iframe></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>训练效果由传入的训练量决定，越大越好
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>假设100000张图片-&gt;共15g大小
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>batch_size 128 表示一次传入的最大参数128张（越大越好）
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>iter 表示完成一次（128张图片）前向传播和反向传播的**迭代**过程
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>epoch  表示全部数据（100000张图片）完成一次**轮次**训练
</span></span></code></pre></td></tr></table></div></div></section><footer class=article-footer><section class=article-tags><a href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a>
<a href=/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/>人工智能</a>
<a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a>
<a href=/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/>大模型</a>
<a href=/tags/ai/>AI</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>自由、和谐、民主、富强 🤯 2025</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/><div class=article-image><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-3.0362959ac97f8729d13c4b0b36b94e92_hu_283c093256923bc8.png width=250 height=150 loading=lazy alt="Featured image of post 人工智能：什么是自然语言处理" data-hash="md5-A2KVmsl/hynRPEsLNrlOkg=="></div><div class=article-details><h2 class=article-title>人工智能：什么是自然语言处理</h2></div></a></article><article class=has-image><a href=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/><div class=article-image><img src=/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-3.f38c2eb2fa13428ff5152a29f7b79025_hu_c0cc08818ce2ab8a.png width=250 height=150 loading=lazy alt="Featured image of post 人工智能：什么是深度学习" data-hash="md5-84wusvoTQo/1FSop97eQJQ=="></div><div class=article-details><h2 class=article-title>人工智能：什么是深度学习</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 八 六</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>