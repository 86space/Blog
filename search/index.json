[{"content":"自然语言处理（NLP）是一门融合了计算机科学、人工智能和语言学的交叉学科，旨在让计算机能够理解、解释和生成人类的自然语言。自然语言是指日常使用的语言，如汉语和英语，它比编程语言更复杂多样。 NLP的目标是使机器能够像人一样理解和使用语言，以实现更自然高效的人机交互。这包括文本信息提取、自动翻译、情感分析、语音识别和问答系统等应用。例如，NLP可以用来开发聊天机器人和翻译软件。 为了实现这些目标，研究人员使用大量数据集和先进算法，如循环神经网络（RNN）、长短时记忆网络（LSTM）和变换器（Transformer）。这些技术的进步显著提升了NLP系统的性能，在许多任务上达到了接近甚至超过人类的水平。\n一个典型的 NLP 流程大致分为以下几个步骤：\n输入 (Text File)： NLP 过程的起始点，通常是文本文件，例如文章、对话、网页等。 这是 NLP 系统的原始数据，例如一段文字：“今天天气真好，适合出去玩。” 预处理 (Preprocessing)： 对原始文本进行清洗和规范化，为后续处理做准备。 目的： 清理和规范化文本，去除噪声，使模型更容易学习。\n常用方法：\n分词 (Tokenization)： 将文本分解成独立的词语或子词。例如，将“今天天气真好” 分成 “今天”，“天气”，“真”，“好”。 去除停用词 (Stop Word Removal)： 去除常用的、没有实际意义的词语，例如“的”，“是”，“了”等。 词干提取 (Stemming) / 词形还原 (Lemmatization)： 将词语还原为原型，例如将“running” 还原为 “run”。 大小写转换 (Lowercasing)： 将所有字母转换为小写，避免大小写带来的差异。 标点符号去除 (Punctuation Removal)： 去除文本中的标点符号。 例子： 经过预处理后，“今天天气真好，适合出去玩。” 可能变为 [\u0026ldquo;今天\u0026rdquo;, \u0026ldquo;天气\u0026rdquo;, \u0026ldquo;真\u0026rdquo;, \u0026ldquo;好\u0026rdquo;, \u0026ldquo;适合\u0026rdquo;, \u0026ldquo;出去玩\u0026rdquo;]。\n词嵌入 (Dense Embedding)： 将文本转换为计算机可以理解的数值表示形式。一个分词的词嵌入里有好几百个值 目的： 将词语转换为计算机可以理解的数值向量。\n原理： 每个词语都被映射到一个高维空间中的向量，向量之间的距离反映了词语之间的语义相似度。\n常用方法：\nWord2Vec： 一种经典的词嵌入方法，通过训练神经网络来学习词向量。 GloVe： 一种基于词语共现统计的词嵌入方法。 FastText： 一种改进的词嵌入方法，可以处理未登录词。 例子： “今天” 可能被表示为 [0.1, 0.2, -0.3, 0.5 \u0026hellip;], “天气” 可能被表示为 [0.2, 0.3, -0.1, 0.4 \u0026hellip;]。\n图中 “Dense Embedding” 下面的蓝色圆点就代表了词向量。\n词嵌入层的作用类似于查字典，将文字转换为计算机可以处理的数字，同时保留了词语的语义信息。\n深度学习 (Deep Learning)： 使用深度神经网络来学习文本的特征和模式。 目的： 利用神经网络来学习文本的深层特征和模式。\n常用模型：\n循环神经网络 (RNN)： 适用于处理序列数据，如文本。 长短期记忆网络 (LSTM)： 改进的 RNN，可以更好地处理长距离依赖关系。 Transformer： 基于自注意力机制的模型，在很多 NLP 任务上都取得了SOTA效果。 图中 “Hidden Layer” 代表了神经网络的隐藏层，它负责学习输入数据的复杂特征。\n“Output Units” 代表了神经网络的输出层，它根据任务的需求输出不同的结果。\n输出 (Output)： 根据不同的任务，输出相应的结果，例如情感分析、文本分类、实体识别、机器翻译等。 目的： 根据不同的 NLP 任务，输出相应的结果。\n常见任务：\n情感分析 (Sentiment)： 判断文本的情感倾向，例如“积极”、“消极”、“中性”。 文本分类 (Classification)： 将文本归类到预定义的类别中。 实体识别 (Entity Extraction)： 识别文本中的命名实体，例如人名、地名、组织机构名。 机器翻译 (Translation)： 将一种语言的文本翻译成另一种语言。 主题建模 (Topic Modeling)： 从文本中提取主题。 词嵌入扩展\n处理分词，模型会生成一个包含多个值的向量（或者说一列数值），并根据每个单词与 work 在训练数据中的距离调整这些值。这个向量就是所谓的词嵌入（Word Embedding）。\n词嵌入里有好几百个值，每个值都代表了单词意义的不同方面。就好比我们用类型、位置、卧室数、浴室数和楼层数来描述房子一样，词嵌入里的值描述了单词的语言特性。\n由于我们无法准确知道每个值代表什么，有意思的是，我们发现那些意思相近的单词，它们的词嵌入往往很像。比如 sea 和 ocean，虽然它们不能在所有情境下互换，但它们的意思很接近，通过词嵌入，我们能量化这种相似度。\n如果我们简化词嵌入，只用两个值来表示，就能更直观地看到单词之间的“距离”或者说相似度了。这样，我们就能发现一些词群，比如 代词群， 交通工具群。而能量化单词，正是让模型成功生成文本的第一步。\n接下来学习最富盛名的自然语言框架：Transformer模型，也就是隐藏层和输出层的内容。\nTransformer模型 Transformer 模型的核心思想是使用**自注意力机制（Self-Attention Mechanism）**来捕捉输入序列中不同位置之间的关系，而不是像 RNN 那样按顺序处理。自注意力机制允许模型同时关注输入序列中的所有位置，从而更好地捕捉长距离依赖关系。\nTransformer的内部，在本质上是一个 encoder-decoder的结构，即编码器-解码器\nTransformer工作流程\n1. 整体结构：\n输入 (Inputs)： 模型的原始输入，例如文本序列。 输入嵌入 (Input Embedding)： 将输入的词语转换为词向量表示。 位置编码 (Positional Encoding)： 为词向量添加位置信息，以表示词语在序列中的顺序。 编码器 (Encoder)： 负责处理输入序列，提取文本的语义信息。 解码器 (Decoder)： 负责根据编码器的输出，生成目标序列。 输出 (Outputs)： 模型的最终输出，例如翻译后的文本，或者分类结果。 输出概率 (Output Probabilities)： 对输出进行概率化处理，以便进行概率预测。 2. 具体步骤详解：\n输入 (Inputs)：\n图中显示输入为 “我 是 一个 学生”，这可以是任何文本序列，例如一句话、一篇文章等。 输入嵌入 (Input Embedding)：\n目的： 将输入的词语转换为计算机可以处理的数值向量，即词向量。 过程： 每个词语通过一个嵌入层（Embedding Layer）映射到一个高维向量空间，例如，“我” 可能被表示为 [0.1, 0.2, -0.3, 0.5 \u0026hellip;]。 图中， “Input Embedding” 模块负责完成这个转换。 位置编码 (Positional Encoding)：\n目的： 由于 Transformer 模型没有 RNN 的顺序处理机制，需要额外的位置编码来表示词语在句子中的位置信息。 过程： 位置编码通过一个特定的函数生成，并与词向量相加，从而将位置信息融入到词向量中。 图中， “Positional Encoding” 模块负责生成位置编码，并与词向量相加。 编码器 (Encoder)：\n目的： 提取输入序列的语义信息。 结构： 由 Nx 个相同的编码器层堆叠而成（图中显示 Nx）。每一层编码器包含： 多头注意力层 (Multi-Head Attention)： 这是 Transformer 模型的核心，它使用自注意力机制来计算输入序列中不同位置之间的关系。 前馈神经网络 (Feed Forward)： 对每个位置的输出进行非线性变换。 加和 \u0026amp; 归一化 (Add \u0026amp; Norm)： 使用残差连接和层归一化来稳定训练过程，加速收敛。 图中，橙色框代表编码器，它接收经过位置编码的输入，并通过多层注意力机制和前馈网络提取特征。 编码器的输出是包含了输入序列语义信息的向量表示。 解码器 (Decoder)：\n目的： 根据编码器的输出，生成目标序列。 结构： 由 Nx 个相同的解码器层堆叠而成（图中显示 Nx）。每一层解码器包含： 掩码多头注意力层 (Masked Multi-Head Attention)： 与编码器的多头注意力层类似，但只关注解码器当前位置之前的信息，避免信息泄露。 多头注意力层 (Multi-Head Attention)： 计算解码器当前位置与编码器输出之间的注意力权重，用于将编码器的信息融入到解码器中。 前馈神经网络 (Feed Forward)： 与编码器中的前馈网络类似。 加和 \u0026amp; 归一化 (Add \u0026amp; Norm)： 与编码器中的残差连接和层归一化类似。 图中，粉色框代表解码器，它接收编码器的输出，并通过多层注意力机制和前馈网络生成目标序列。 解码器是自回归的，它逐词生成目标序列。 输出嵌入 (Output Embedding)：\n目的： 将目标序列的词语转换为词向量。 过程： 与输入嵌入类似，每个目标词语通过一个嵌入层映射到一个高维向量空间。 图中，“Output Embedding” 模块负责目标词语的向量化。 在训练过程中，解码器的输入是目标序列，但向右移动一位，第一个输入是起始符 **\u0026lt;start\u0026gt;**，最后一个输入是结束符 **\u0026lt;end\u0026gt;**。 线性层 (Linear)：\n目的： 将解码器的输出转换为与词汇表大小相同的向量，用于计算每个词语的概率。 过程： 通过一个线性变换，将解码器的输出映射到一个词汇表大小的空间。 Softmax：\n目的： 将线性层的输出转换为概率分布，用于预测下一个词语。 过程： Softmax 函数将线性层的输出转换为一个概率分布，每个位置的值表示该位置是词汇表中某个词语的概率。 输出概率 (Output Probabilities)：\n目的： 表示模型预测的下一个词语的概率分布。 图中，输出显示了生成目标序列 “i am a student” 的过程，每个词语都对应一个概率分布。 例如，第一个输出 “i” 的概率最高，第二个输出 “am” 的概率最高，以此类推。 ","date":"2025-02-07T11:43:04Z","image":"https://blog.8688886.xyz/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/image-3_hu_b2e130c9d4262008.png","permalink":"https://blog.8688886.xyz/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/","title":"人工智能：什么是自然语言处理"},{"content":"人工神经网络（Artificial Neural Network ，简写：ANN）是模仿生物神经系统（特别是人脑）结构和功能的一种计算模型。它们是深度学习的核心组成部分，赋予了深度学习强大的模式识别和学习能力。\n神经网络的基本结构：\n一个基本的神经网络由以下几个部分组成：\n神经元（Neuron）： 也称为节点，是神经网络的基本单元。每个神经元接收输入，进行某种处理，然后产生输出。\n连接（Connection）： 神经元之间通过连接相互传递信息。每个连接都有一个权重（Weight），表示连接的强度。权重决定了输入信号对输出的影响程度。\n层（Layer）： 神经元按层组织。一个典型的神经网络包括：\n输入层（Input Layer）： 接收外部输入数据。 隐藏层（Hidden Layer）： 位于输入层和输出层之间，负责对输入数据进行处理和转换。深度学习中的“深度”就体现在这里，拥有多个隐藏层。 输出层（Output Layer）： 产生最终的输出结果。 特点：\n同一层的神经元之间没有连接。 第N层的每个神经元和第N-1层的所有神经元相连（这就是full connection的含义），也称为全连接神经网络。 第N-1层神经元的输出就是第N层神经元的输入。 每个连接都有一个权重值（w系数和b系数）。 神经网络的工作原理：\n输入： 输入数据被传递到输入层的神经元。 加权和： 每个神经元接收来自上一层神经元的输入，并将这些输入乘以相应的权重进行加权求和。 激活函数： 将加权和传递给一个激活函数。激活函数引入了非线性，使得神经网络能够学习复杂的模式。常见的激活函数包括Sigmoid、ReLU、Tanh等。 输出： 激活函数的输出成为该神经元的输出，并传递到下一层神经元。 重复： 这个过程在网络中逐层进行，直到到达输出层。 训练神经网络：\n训练神经网络的过程包括以下几个步骤：\n前向传播（Forward Propagation）：将输入数据通过网络各层传递，计算每个神经元的输出，直到产生最终输出。 损失函数（Loss Function）：计算预测输出和实际标签之间的误差。这种误差称为损失，常见的损失函数有均方误差（MSE）和交叉熵损失（Cross-Entropy Loss）。 反向传播（Backpropagation）：通过损失函数的梯度计算，反向调整每个神经元的权重，以最小化损失。反向传播算法使用链式法则来高效计算梯度。 优化器（Optimizer）：使用优化算法（如梯度下降、Adam 等）根据计算出的梯度更新网络权重，从而减小损失。 激活函数 加权和在进行输入计算后始终是线性的，需要引入激活函数用于对每层的输出数据进行变换，进而为整个网络注入非线性因素，此时神经网络可以拟合出各种曲线，以此来表达复杂问题的解答。\nPytorch中封装了常见的激活函数\nsigmoid 激活函数\u0026ndash; 指数型\nsigmoid函数一般只用于二分类的输出层\nSigmoid函数，也称为S型函数或乙状函数，是一种形状像字母“S”的数学函数。它在机器学习，特别是深度学习和逻辑回归中，作为激活函数被广泛使用。\n定义和公式：\nSigmoid函数最常见的形式是逻辑斯谛函数，其公式如下：\nσ(x) = 1 / (1 + e-x)\n其中：\nσ(x) 是函数的输出值。 x 是函数的输入值（可以是任意实数）。 e 是自然常数（约等于2.71828）。 图像和特性：\nSigmoid函数的图像呈S形，具有以下特性：\n定义域： (−∞, +∞) （输入可以是任意实数） 值域： (0, 1) （输出值始终在0和1之间，但不包括0和1） 单调递增： 输入值越大，输出值也越大。 连续光滑： 函数曲线平滑连续，处处可导。 在x=0处中心对称： σ(0) = 0.5。 导数： σ\u0026rsquo;(x) = σ(x)(1 - σ(x))，导数可以用函数自身来表示，这在反向传播算法中非常有用。 Sigmoid函数在神经网络中的应用：\n在神经网络中，Sigmoid函数通常作为激活函数使用，其主要作用是引入非线性。如果没有激活函数，神经网络就只能进行线性运算，表达能力非常有限。Sigmoid函数将神经元的加权输入转换为0到1之间的输出，可以看作是神经元“激活”的概率。\nSigmoid函数的优缺点：\n优点：\n输出范围有限： 将输出限制在0到1之间，可以方便地表示概率或进行归一化。 光滑可导： 方便进行梯度计算，用于反向传播算法。 易于理解和使用： 函数形式简单，易于实现。 缺点：\n梯度消失： 当输入值非常大或非常小时，函数的梯度接近于0。这会导致在反向传播过程中，梯度难以传递到前面的层，从而导致网络训练缓慢甚至停滞。这是Sigmoid函数最大的缺点，也是它在深度神经网络中逐渐被其他激活函数（如ReLU）取代的主要原因。 输出不是以0为中心： 输出值始终为正，这可能会导致一些优化问题。 计算量较大： 计算指数运算相对耗时。 ReLU激活函数-分段型\nReLu函数常用于隐藏层\n定义和公式：\nReLU函数的数学表达式非常简单：\nf(x) = max(0, x)\n这意味着：\n当输入x大于0时，输出等于输入本身（f(x) = x）。 当输入x小于等于0时，输出为0。 图像和特性：\nReLU函数的图像由两条直线组成，在x=0处有一个拐点。其主要特性包括：\n非线性： ReLU函数虽然形式简单，但它是一个非线性函数，这使得神经网络能够学习复杂的模式。 计算简单： ReLU函数只需要进行简单的比较和赋值操作，计算速度非常快。 单侧抑制： 当输入小于0时，输出恒为0，这导致一部分神经元处于“非激活”状态，有助于网络的稀疏性表达，减少参数之间的相互依赖，缓解过拟合。 不存在梯度消失问题（在正区间）： 当输入大于0时，梯度恒为1，这使得梯度可以有效地传递到前面的层，避免了梯度消失问题，加快了网络训练速度。 ReLU函数的优缺点：\n优点：\n缓解梯度消失问题： 这是ReLU最重要的优点之一，它有效地解决了Sigmoid和tanh等激活函数在输入较大或较小时容易出现的梯度消失问题，使得深度神经网络更容易训练。 计算速度快： 相比于Sigmoid和tanh等需要进行指数运算的激活函数，ReLU的计算速度非常快，这大大加快了网络的训练速度。 促进稀疏性： ReLU的单侧抑制特性使得一部分神经元输出为0，从而产生稀疏的网络结构，有助于减少参数之间的相互依赖，提高模型的泛化能力。 缺点：\nDead ReLU Problem（神经元死亡问题）： 如果某个神经元的输入在训练过程中一直为负数，那么该神经元的输出将始终为0，导致该神经元“死亡”，不再对网络的学习起到作用。这通常是由于较大的学习率或不合适的初始化参数导致的。 ReLU的变体：\n为了解决Dead ReLU Problem，人们提出了一些ReLU的变体，例如：\nLeaky ReLU（带泄漏的ReLU）： 将输入小于0的部分赋予一个很小的斜率，而不是直接设为0。例如，f(x) = x (x\u0026gt;0)；f(x) = αx (x\u0026lt;=0)，其中α是一个很小的常数，例如0.01。这避免了神经元完全“死亡”的情况。 Parametric ReLU (PReLU，参数化ReLU)： 将Leaky ReLU中的α作为一个可学习的参数，通过反向传播进行学习。 Exponential Linear Unit (ELU，指数线性单元)： 在输入小于0的部分使用指数函数，而不是简单的线性函数。 ReLU在神经网络中的应用：\nReLU及其变体是目前深度学习中最常用的激活函数之一，尤其在卷积神经网络（CNN）中应用广泛。通常来说，ReLU是隐藏层激活函数的首选。\nSoftMax激活函数\n常用于处理多分类问题的输出层，将多分类结果以概率的形式展示。\n定义和公式：\n给定一个包含 n 个实数的向量 z = (z1, z2, \u0026hellip;, zn)，Softmax 函数的定义如下：\nSoftmax(z)i = ezi / Σj=1n ezj\n其中：\nSoftmax(z)i 表示向量 z 的第 i 个元素的 Softmax 输出。 e 是自然常数（约等于 2.71828）。 Σj=1n ezj 表示对向量 z 的所有元素的指数求和。 工作原理：\nSoftmax 函数首先对输入向量的每个元素取指数，然后将每个元素的指数值除以所有元素指数值的总和。这样就保证了输出向量的每个元素都在 (0, 1) 之间，并且所有元素的总和为 1，从而构成了一个概率分布。\nSoftmax 在神经网络中的应用：\nSoftmax 函数通常用于神经网络的输出层，尤其是在多分类问题中。例如，在图像分类任务中，神经网络的输出层可以有 10 个神经元，分别对应 10 个不同的类别。经过 Softmax 函数处理后，输出层每个神经元的输出就表示该图像属于对应类别的概率。\nSoftmax 的优点：\n输出为概率分布： Softmax 的输出可以直接解释为概率，方便进行分类决策。 突出最大值： Softmax 函数能够突出输入向量中值最大的元素，使得分类结果更加明确。 Softmax 的缺点：\n计算开销： 需要计算指数和求和，计算开销相对较大。 梯度消失（在某些情况下）： 当输入向量的某些元素值非常大时，可能会导致梯度消失问题。 对输入变化敏感： 输入向量的微小变化可能会导致输出概率的较大变化。 Softmax 与其他激活函数的比较：\n与 Sigmoid 的比较： Sigmoid 函数通常用于二分类问题，而 Softmax 函数则用于多分类问题。当类别数为 2 时，Softmax 函数退化为 Sigmoid 函数。 与 ReLU 的比较： ReLU 函数主要用于隐藏层，而 Softmax 函数主要用于输出层。ReLU 解决了 Sigmoid 的梯度消失问题，但 Softmax 仍然在多分类输出层占有重要地位。 总结：\nSoftmax 激活函数是一种重要的激活函数，尤其在多分类问题中应用广泛。它将输入向量转换为概率分布，方便进行分类决策。虽然存在一些缺点，但仍然是深度学习中不可或缺的一部分。\n补充说明：\n为了数值稳定性，在实际应用中，通常会对 Softmax 函数的计算进行一些改进，例如减去输入向量的最大值。 Softmax 函数通常与交叉熵损失函数一起使用，以优化神经网络的训练。 示例：\n假设有一个输入向量 z = [2, 1, 0]，则 Softmax 的计算过程如下：\n计算每个元素的指数： e2 ≈ 7.39 e1 ≈ 2.72 e0 = 1 计算所有元素指数值的总和： 7.39 + 2.72 + 1 = 11.11 计算每个元素的 Softmax 输出： Softmax(z)1 = 7.39 / 11.11 ≈ 0.665 Softmax(z)2 = 2.72 / 11.11 ≈ 0.245 Softmax(z)3 = 1 / 11.11 ≈ 0.090 因此，Softmax(z) ≈ [0.665, 0.245, 0.090]。可以看到，这三个数的和接近于 1，并且每个数都在 0 到 1 之间。\n激活函数的选择方法 对于隐藏层：\n优先选择ReLU激活函数 如果relu效果不好，那么尝试其他激活，如leaky ReLU等。 如果你使用了ReLU,需要注意一下dead ReLU问题，避免出现大的梯度从而导致过多的神经元死亡。 对于输出层：\n二分类问题选择sigmoid激活函数 多分类问题选择SoftMax激活函数 其他常见的激活函数\n损失函数 什么是损失函数\n用来衡量模型参数质量的函数，衡量的方式是比较神经网络输出和真实输出的差异，**损失函数告诉我们模型“犯了多大的错误”。通过最小化损失函数，我们可以优化模型的参数，使其预测结果更接近真实值。**例如：输入一张猫的照片，看输出的结果，\n损失函数的作用：\n指导模型训练： 在训练过程中，优化算法（如梯度下降）会根据损失函数的梯度来调整模型参数，从而使损失函数的值不断减小。 评估模型性能： 在训练完成后，我们可以使用损失函数来评估模型在测试集上的表现，从而了解模型的泛化能力。 常见的损失函数：\n损失函数可以根据任务类型进行分类，主要分为以下两类：\n回归问题损失函数： 用于预测连续值的任务。 分类问题损失函数： 用于预测离散类别的任务。 分类问题损失函数\n交叉熵损失（Cross-Entropy Loss）： 常用于多分类问题。\n在多分类任务通常使用softmax将logits转换为概率的形式\nH(p, q) = - Σp(x)log(q(x))\n其中，p(x) 是真实概率分布，q(x) 是预测概率分布。\n优点： 能够有效地衡量两个概率分布之间的差异，优化效果好。 损失函数与激活函数的关系：\n损失函数和激活函数是深度学习模型中两个重要的组成部分，它们之间存在一定的联系。例如，在多分类问题中，通常使用 Softmax 激活函数将输出转换为概率分布，然后使用交叉熵损失函数来衡量预测结果与真实结果之间的差异。\n网络优化方法 梯度下降算法是一种用于寻找损失函数最小值的优化算法\n核心思想：\n想象你站在一座山上，想要下到山谷。梯度下降法就像你在山上寻找最陡峭的下坡路，然后沿着这个方向走一步，重复这个过程，直到到达山谷的最低点。\n数学原理：\n梯度： 在数学中，梯度是一个向量，它指向函数值增长最快的方向。因此，梯度的反方向就是函数值下降最快的方向。 迭代： 梯度下降法通过不断迭代来逼近最小值。每次迭代，它都会沿着梯度的反方向移动一小步。 公式：\nθnew = θold - α * ∇J(θ)\n其中：\nθnew：更新后的参数值。 θold：当前的参数值。 α：学习率（learning rate），控制每次迭代的步长。 ∇J(θ)：损失函数 J(θ) 关于参数 θ 的梯度。 步骤：\n初始化参数： 随机初始化模型的参数 θ。 计算梯度： 计算损失函数 J(θ) 关于参数 θ 的梯度 ∇J(θ)。 更新参数： 使用上述公式更新参数 θ。 重复步骤 2 和 3： 直到损失函数的值收敛到一个可接受的范围内，或者达到预定的迭代次数。 不同类型的梯度下降：\n根据每次迭代使用的样本数量，梯度下降法可以分为以下几种类型：\n批量梯度下降（Batch Gradient Descent, BGD）： 每次迭代使用所有训练样本来计算梯度。\n优点： 能够保证收敛到全局最小值（对于凸函数），训练过程相对稳定。 缺点： 计算量大，训练速度慢，不适合处理大规模数据集。 随机梯度下降（Stochastic Gradient Descent, SGD）： 每次迭代只使用一个随机选择的训练样本来计算梯度。\n优点： 计算速度快，适合处理大规模数据集，有可能跳出局部最小值。 缺点： 训练过程波动较大，不容易收敛到全局最小值。 小批量梯度下降（Mini-batch Gradient Descent, MBGD）： 每次迭代使用一小部分随机选择的训练样本（称为一个 mini-batch）来计算梯度。\n优点： 结合了 BGD 和 SGD 的优点，既能保证一定的稳定性，又能加快训练速度。 缺点： 需要选择合适的 mini-batch 大小。 学习率的选择：\n学习率 α 是梯度下降法中一个重要的超参数，它控制了每次迭代的步长。\n学习率过大： 可能导致算法在最小值附近震荡，无法收敛。 学习率过小： 可能导致收敛速度过慢。 通常需要通过实验来选择合适的学习率。常用的方法包括：\n试错法： 尝试不同的学习率，观察训练效果。 学习率衰减： 随着训练的进行，逐渐减小学习率。 总结：\n梯度下降法是一种简单而有效的优化算法，在机器学习和深度学习中有着广泛的应用。理解其基本原理、不同类型以及学习率的选择对于有效地使用梯度下降法至关重要。\n补充说明：\n除了基本的梯度下降法，还有许多改进的优化算法，例如：\n动量法（Momentum）： 引入动量来加速收敛，并减少震荡。 Adam 优化器： 结合了动量法和 RMSProp 的优点，是一种常用的自适应优化算法。 这些改进的优化算法通常能够更快地收敛到最小值，并且对学习率的选择不那么敏感。\n前向传播和反向传播 前向传播 传参正向计算的过程\n反向传播 参数优化的过程，调整权重参数weight，先从深层开始，逐步到浅层。\n前向传播和反向传播视频教程\n1 2 3 4 5 6 7 8 9 训练效果由传入的训练量决定，越大越好 假设100000张图片-\u0026gt;共15g大小 batch_size 128 表示一次传入的最大参数128张（越大越好） iter 表示完成一次（128张图片）前向传播和反向传播的**迭代**过程 epoch 表示全部数据（100000张图片）完成一次**轮次**训练 ","date":"2025-02-07T11:30:28Z","image":"https://blog.8688886.xyz/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-1_hu_3165bb533d8c5e60.png","permalink":"https://blog.8688886.xyz/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","title":"人工智能：什么是神经网络"},{"content":"什么是深度学习 机器学习是实现人工智能的一种途径。 深度学习是机器学习的一个子集，也就是一个具体的方法。\n人工智能（AI）、机器学习（ML）和深度学习（DL）是密切相关的概念，它们之间的关系可以用一个包含关系来表示：人工智能 \u0026gt; 机器学习 \u0026gt; 深度学习。\n1. 人工智能 (AI)\n人工智能是计算机科学的一个广阔领域，其目标是创造能够执行通常需要人类智能的任务的机器。这些任务包括：\n问题解决： 制定策略和算法来解决复杂问题。 学习： 从经验中学习并改进性能。 推理： 使用逻辑和规则得出结论。 感知： 通过传感器（例如，摄像头、麦克风）感知环境。 语言理解： 理解和生成人类语言。 人工智能是一个非常广泛的概念，它涵盖了许多不同的技术和方法。\n2. 机器学习 (ML)\n机器学习是人工智能的一个子领域。它专注于开发允许计算机无需明确编程即可从数据中学习的算法和技术。换句话说，机器学习算法通过分析大量数据来识别模式、做出预测或做出决策，而无需人工编写具体的规则。\n机器学习的常见类型包括：\n监督学习： 使用带有标签的训练数据（即输入和正确的输出）来训练模型。例如，使用图像和对应的标签（例如，“猫”、“狗”）来训练图像分类器。 无监督学习： 使用没有标签的训练数据来发现数据中的隐藏模式。例如，使用客户购买数据来识别不同的客户群体。 强化学习： 训练智能体在环境中采取行动，以最大化某种奖励。例如，训练一个游戏AI来赢得比赛。 3. 深度学习 (DL)\n深度学习是机器学习的一个子集。它使用人工神经网络，特别是具有多个层（因此称为“深度”）的神经网络，来处理和分析数据。这些深层神经网络能够学习非常复杂的模式，并且在图像识别、语音识别和自然语言处理等任务中表现出色。\n深度学习的关键概念是：\n神经网络： 受人脑结构启发的计算模型，由相互连接的节点（称为神经元）组成。 深度神经网络 (DNN)： 具有多个隐藏层的神经网络。这些隐藏层允许网络学习数据的分层表示。 反向传播： 一种用于训练神经网络的算法，通过调整网络中连接的权重来最小化预测误差。 它们之间的关系总结：\n人工智能是一个总体的概念，涵盖了所有使机器能够模拟人类智能的技术。 机器学习是实现人工智能的一种方法，它侧重于使机器能够从数据中学习。 深度学习是机器学习的一个子集，它使用深层神经网络来实现更高级的学习和模式识别。 一个简单的类比：\n想象一下你想制造一辆自动驾驶汽车。\n人工智能 是整个自动驾驶汽车项目的目标，包括感知环境、规划路线、控制车辆等。 机器学习 是一种方法，用于训练汽车的感知系统，例如通过分析大量图像来识别道路标志和行人。 深度学习 是一种特定的机器学习技术，用于构建汽车的图像识别系统，它使用深层神经网络来处理图像数据并进行准确的识别。 深度学习与机器学习的区别 机器学习需要依赖人工提取特征，而深度学习不需要进行人工特征提取，只需要把神经网络搭建完成。\n网络结构的设计就是搭建神经网络的过程。\n发展历程 重点是Tranformer和chatGPT的出现。\n","date":"2025-01-22T16:06:24Z","image":"https://blog.8688886.xyz/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-3_hu_31a67bc170d5bcee.png","permalink":"https://blog.8688886.xyz/p/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","title":"人工智能：什么是深度学习"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] One line code block 1 \u0026lt;p\u0026gt;A paragraph\u0026lt;/p\u0026gt; List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL + ALT + Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-09-07T00:00:00Z","permalink":"https://blog.8688886.xyz/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Hugo theme Stack supports the creation of interactive image galleries using Markdown. It\u0026rsquo;s powered by PhotoSwipe and its syntax was inspired by Typlog.\nTo use this feature, the image must be in the same directory as the Markdown file, as it uses Hugo\u0026rsquo;s page bundle feature to read the dimensions of the image. External images are not supported.\nSyntax 1 ![Image 1](1.jpg) ![Image 2](2.jpg) Result Photo by mymind and Luke Chesser on Unsplash\n","date":"2023-08-26T00:00:00Z","image":"https://blog.8688886.xyz/p/image-gallery/2_hu_3e58a979f20e4e46.jpg","permalink":"https://blog.8688886.xyz/p/image-gallery/","title":"Image gallery"},{"content":"For more details, check out the documentation.\nBilibili video Tencent video YouTube video Generic video file Your browser doesn't support HTML5 video. Here is a link to the video instead. Gist GitLab Quote Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n― A famous person, The book they wrote Photo by Codioful on Unsplash\n","date":"2023-08-25T00:00:00Z","image":"https://blog.8688886.xyz/p/shortcodes/cover_hu_5667347daefb4230.jpg","permalink":"https://blog.8688886.xyz/p/shortcodes/","title":"Shortcodes"},{"content":"Stack has built-in support for math typesetting using KaTeX.\nIt\u0026rsquo;s not enabled by default side-wide, but you can enable it for individual posts by adding math: true to the front matter. Or you can enable it side-wide by adding math = true to the params.article section in config.toml.\nInline math This is an inline mathematical expression: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$\n1 $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$ Block math $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$ 1 2 3 $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$ $$ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi $$ 1 2 3 $$ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi $$ ","date":"2023-08-24T00:00:00Z","permalink":"https://blog.8688886.xyz/p/math-typesetting/","title":"Math Typesetting"},{"content":"Welcome to Hugo theme Stack. This is your first post. Edit or delete it, then start writing!\nFor more information about this theme, check the documentation: https://stack.jimmycai.com/\nWant a site like this? Check out hugo-theme-stack-stater\nPhoto by Pawel Czerwinski on Unsplash\n","date":"2022-03-06T00:00:00Z","image":"https://blog.8688886.xyz/p/hello-world/cover_hu_e95a4276bf860a84.jpg","permalink":"https://blog.8688886.xyz/p/hello-world/","title":"Hello World"}]